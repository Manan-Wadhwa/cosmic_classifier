{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho8u_Az_NKEg",
        "outputId": "a6fa280a-c36a-4c7b-d51e-89a31d93a9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Process Started: Data Loading and Preprocessing ===\n",
            "Loading data from 'cosmicclassifierTraining.csv'...\n",
            "Data loaded with shape: (60000, 11)\n",
            "Replacing sentinel values (-999) with NaN...\n",
            "Converting categorical columns if needed...\n",
            "Dropping rows with missing target variable 'Prediction'...\n",
            "Separating features and target...\n",
            "Imputing missing values in features using median strategy...\n",
            "Imputation complete. Final feature shape: (56961, 10)\n",
            "=== Data Loading and Preprocessing Completed ===\n",
            "\n",
            "=== Running Feature Diagnostics ===\n",
            "Generating class distribution plot...\n",
            "Class distribution plot saved as 'class_distribution.png'.\n",
            "Generating correlation heatmap...\n",
            "Correlation heatmap saved as 'correlation_heatmap.png'.\n",
            "Calculating feature importance using RandomForestClassifier...\n",
            "Feature importance plot saved as 'feature_importances.png'.\n",
            "=== Feature Diagnostics Completed ===\n",
            "\n",
            "=== Splitting Data into Training and Validation Sets ===\n",
            "Training set shape: (45568, 10) | Validation set shape: (11393, 10)\n",
            "\n",
            "=== Applying SMOTE to Balance the Training Data ===\n",
            "SMOTE applied. Resampled training set shape: (51140, 10)\n",
            "\n",
            "=== Defining Base Models for Stacking Classifier ===\n",
            "=== Creating Stacking Classifier with cv=3 for faster training ===\n",
            "=== Scaling Data ===\n",
            "Data scaling complete.\n",
            "\n",
            "=== Training Stacking Classifier ===\n",
            "Training complete.\n",
            "\n",
            "=== Making Predictions on Validation Set ===\n",
            "\n",
            "--- MODEL PERFORMANCE ---\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.93      0.94      1127\n",
            "         1.0       0.96      0.97      0.96      1279\n",
            "         2.0       0.90      0.93      0.91      1129\n",
            "         3.0       0.88      0.83      0.86      1163\n",
            "         4.0       0.86      0.85      0.85      1111\n",
            "         5.0       0.85      0.87      0.86      1026\n",
            "         6.0       0.93      0.92      0.92      1128\n",
            "         7.0       0.91      0.92      0.91      1186\n",
            "         8.0       0.83      0.85      0.84      1114\n",
            "         9.0       0.82      0.80      0.81      1130\n",
            "\n",
            "    accuracy                           0.89     11393\n",
            "   macro avg       0.89      0.89      0.89     11393\n",
            "weighted avg       0.89      0.89      0.89     11393\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1051    2    6    6    0   31   21    2    7    1]\n",
            " [   0 1236   14    5    4    2    5    7    6    0]\n",
            " [   9    0 1048    5   15    4    7   15   25    1]\n",
            " [   6   11   29  970    5   38    3   17   73   11]\n",
            " [   2    2    9    1  945   10   19   12   13   98]\n",
            " [  20    5    4   40    9  894   11    3   25   15]\n",
            " [  16    7   27    2   11   17 1034    0   10    4]\n",
            " [   1    9   10    2   13    6    0 1089   11   45]\n",
            " [   7   10   16   46    6   39   10    7  952   21]\n",
            " [   5    3    5   26   95   11    3   50   31  901]]\n",
            "Overall Accuracy: 0.8883\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def load_and_preprocess_data(train_path):\n",
        "    print(f\"Loading data from '{train_path}'...\")\n",
        "    data = pd.read_csv(train_path)\n",
        "    print(f\"Data loaded with shape: {data.shape}\")\n",
        "\n",
        "    print(\"Replacing sentinel values (-999) with NaN...\")\n",
        "    data.replace({-999: np.nan}, inplace=True)\n",
        "\n",
        "    print(\"Converting categorical columns if needed...\")\n",
        "    category_columns = ['Magnetic Field Strength', 'Radiation Levels']\n",
        "    for col in category_columns:\n",
        "        if col in data and data[col].dtype == 'object':\n",
        "            data[col] = data[col].str.replace('Category_', '').astype(float)\n",
        "\n",
        "    print(\"Dropping rows with missing target variable 'Prediction'...\")\n",
        "    data.dropna(subset=['Prediction'], inplace=True)\n",
        "\n",
        "    print(\"Separating features and target...\")\n",
        "    X = data.drop(columns=['Prediction'])\n",
        "    y = data['Prediction']\n",
        "\n",
        "    print(\"Imputing missing values in features using median strategy...\")\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "    print(f\"Imputation complete. Final feature shape: {X.shape}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def feature_diagnostics(X, y):\n",
        "    print(\"Generating class distribution plot...\")\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    y.value_counts(normalize=True).plot(kind='bar', title='Class Distribution')\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Proportion')\n",
        "    plt.savefig('class_distribution.png')\n",
        "    plt.close()\n",
        "    print(\"Class distribution plot saved as 'class_distribution.png'.\")\n",
        "\n",
        "    print(\"Generating correlation heatmap...\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(X.corr(), annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Feature Correlation Heatmap')\n",
        "    plt.savefig('correlation_heatmap.png')\n",
        "    plt.close()\n",
        "    print(\"Correlation heatmap saved as 'correlation_heatmap.png'.\")\n",
        "\n",
        "    print(\"Calculating feature importance using RandomForestClassifier...\")\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf.fit(X, y)\n",
        "    feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})\\\n",
        "                         .sort_values('Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "    plt.title('Feature Importances')\n",
        "    plt.savefig('feature_importances.png')\n",
        "    plt.close()\n",
        "    print(\"Feature importance plot saved as 'feature_importances.png'.\")\n",
        "\n",
        "def main():\n",
        "    print(\"=== Process Started: Data Loading and Preprocessing ===\")\n",
        "    X, y = load_and_preprocess_data(\"cosmicclassifierTraining.csv\")\n",
        "    print(\"=== Data Loading and Preprocessing Completed ===\\n\")\n",
        "\n",
        "    print(\"=== Running Feature Diagnostics ===\")\n",
        "    feature_diagnostics(X, y)\n",
        "    print(\"=== Feature Diagnostics Completed ===\\n\")\n",
        "\n",
        "    print(\"=== Splitting Data into Training and Validation Sets ===\")\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training set shape: {X_train.shape} | Validation set shape: {X_val.shape}\\n\")\n",
        "\n",
        "    print(\"=== Applying SMOTE to Balance the Training Data ===\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "    print(f\"SMOTE applied. Resampled training set shape: {X_train_resampled.shape}\\n\")\n",
        "\n",
        "    print(\"=== Defining Base Models for Stacking Classifier ===\")\n",
        "    base_models = [\n",
        "        ('rf', RandomForestClassifier(random_state=42, n_estimators=50, n_jobs=-1)),  # Reduced estimators for speed\n",
        "        ('gb', GradientBoostingClassifier(random_state=42)),  # Default settings\n",
        "        ('svm', SVC(random_state=42))  # Removed probability=True to speed up training\n",
        "    ]\n",
        "\n",
        "    print(\"=== Creating Stacking Classifier with cv=3 for faster training ===\")\n",
        "    stacking_classifier = StackingClassifier(\n",
        "        estimators=base_models,\n",
        "        final_estimator=LogisticRegression(multi_class='ovr', max_iter=1000),\n",
        "        cv=3  # Fewer folds for quicker training\n",
        "    )\n",
        "\n",
        "    print(\"=== Scaling Data ===\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    print(\"Data scaling complete.\\n\")\n",
        "\n",
        "    print(\"=== Training Stacking Classifier ===\")\n",
        "    stacking_classifier.fit(X_train_scaled, y_train_resampled)\n",
        "    print(\"Training complete.\\n\")\n",
        "\n",
        "    print(\"=== Making Predictions on Validation Set ===\")\n",
        "    y_pred = stacking_classifier.predict(X_val_scaled)\n",
        "\n",
        "    print(\"\\n--- MODEL PERFORMANCE ---\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_val, y_pred))\n",
        "    print(f\"Overall Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}